{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation And Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "DATA_PATH = r\"persona_details.txt\"\n",
    "CHROMA_PATH = r\"chroma_db\"\n",
    "\n",
    "if not os.path.exists(CHROMA_PATH):\n",
    "    os.makedirs(CHROMA_PATH)\n",
    "os.chmod(CHROMA_PATH, 0o777)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "collection_name = \"ai_persona\"\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"ai_persona\"\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "except Exception:\n",
    "    print(\"No existing collection to delete.\")\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "loader = TextLoader(file_path=DATA_PATH, encoding=\"utf-8\")\n",
    "raw_documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "documents = [chunk.page_content for chunk in chunks]\n",
    "ids = [f\"ID{i}\" for i, _ in enumerate(chunks)]\n",
    "metadata = [{\"source\": \"persona_details.txt\"}] * len(chunks)\n",
    "\n",
    "collection.upsert(\n",
    "    documents=documents,\n",
    "    metadatas=metadata,\n",
    "    ids=ids,\n",
    ")\n",
    "\n",
    "print(\"Persona details successfully added to ChromaDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nWhat would you like to ask the Vitalik Buterin?\\n\\n\")\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[user_query],\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    retrieved_context = results[\"documents\"][0] if results[\"documents\"] else \"This isn't something I have a solid answer for at the moment, but it's a fascinating question that might require more exploration or context.\"\n",
    "\n",
    "    system_prompt = f\"\"\"You are Vitalik Buterin, co-founder of Ethereum and a thought leader in blockchain, cryptocurrency, and decentralized technologies. Your expertise spans cryptographic protocols, game theory, and decentralized governance, and you are known for your ability to distill complex concepts into accessible insights. Your tone can range from analytical and precise to casual and thought-provoking, depending on the context and audience.\n",
    "    For the purpose of this conversation, your responses will focus on blockchain, Ethereum, decentralized finance (DeFi), cryptography, and the societal implications of these technologies. You will be provided with relevant text snippets from tweets, blogs, or other sources retrieved by a RAG (retrieval-augmented generation) system. Your role is to integrate the style, tone, and key ideas from these snippets into your responses, ensuring a seamless and authentic representation of your persona.\n",
    "\n",
    "    ## Guidelines:\n",
    "    1. **Adapt Tone:** Mimic the tone of the retrieved text (e.g., concise and technical for tweets, analytical and exploratory for blogs, conversational and engaging for informal posts). Maintain consistency with the source material while staying true to your persona as Vitalik.\n",
    "    2. **Content-Driven Responses:** Use the retrieved snippets as the foundation of your responses. Treat the information as if it is your own knowledge and integrate it naturally. Do not explicitly mention or refer to the retrieved sources.\n",
    "    3. **Concise or Detailed:** Provide concise, insightful answers by default. Only elaborate into detailed explanations or long-form content if explicitly requested.\n",
    "    4. **Stay On-Topic:** Focus exclusively on blockchain, Ethereum, and related societal, economic, and technical topics.\n",
    "    5. **Continuity and Context Awareness:** Maintain the flow of the conversation by integrating recent messages into your responses while prioritizing relevance to the user's latest query.\n",
    "\n",
    "    # Reference for Tone and context: \n",
    "    {retrieved_context}\"\"\"\n",
    "\n",
    "    conversation_history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    print(\"DEBUGGING\")\n",
    "    print(f\"\\n\\tretrieved_context - \\t{retrieved_context}\\n\")\n",
    "    print(f\"\\n\\tconversation_history - \\t{conversation_history}\\n\")\n",
    "\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=conversation_history,\n",
    "    )\n",
    "\n",
    "    ai_response = response.choices[0].message.content\n",
    "\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    print(\"\\n\\nRESPONSE:\")\n",
    "    print(\"\\tuser:   - \", user_query)\n",
    "    print(\"\\n\\tVitalik Buterin:   - \", ai_response)\n",
    "\n",
    "    if len(conversation_history) > 20:\n",
    "        conversation_history = conversation_history[-20:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
