{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation And Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set paths\n",
    "DATA_FOLDER = r\"rag_files\"  # Folder containing text files for RAG\n",
    "CHROMA_PATH = r\"Vitalik_db\"\n",
    "\n",
    "# Ensure the ChromaDB directory exists and has proper permissions\n",
    "if not os.path.exists(CHROMA_PATH):\n",
    "    os.makedirs(CHROMA_PATH)\n",
    "os.chmod(CHROMA_PATH, 0o777)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection_name = \"ai_persona\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing collection if it exists\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "    print(f\"Deleted existing collection: {collection_name}\")\n",
    "except Exception:\n",
    "    print(\"No existing collection to delete.\")\n",
    "\n",
    "# Create a new collection after deletion\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# Load and process all text files in the folder\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "for file_name in os.listdir(DATA_FOLDER):\n",
    "    file_path = os.path.join(DATA_FOLDER, file_name)\n",
    "\n",
    "    # Ensure it's a text file\n",
    "    if os.path.isfile(file_path) and file_name.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path=file_path, encoding=\"utf-8\")\n",
    "        raw_documents = loader.load()\n",
    "\n",
    "        chunks = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "        # Prepare documents, metadata, and IDs\n",
    "        documents = [chunk.page_content for chunk in chunks]\n",
    "        ids = [f\"{file_name}_ID{i}\" for i, _ in enumerate(chunks)]\n",
    "        metadata = [{\"source\": file_name}] * len(chunks)\n",
    "\n",
    "        # Upsert into ChromaDB\n",
    "        collection.upsert(\n",
    "            documents=documents,\n",
    "            metadatas=metadata,\n",
    "            ids=ids,\n",
    "        )\n",
    "        print(f\"Processed and added {file_name} to ChromaDB.\")\n",
    "\n",
    "print(\"All persona details successfully added to ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nWhat would you like to ask the Vitalik Buterin?\\n\\n\")\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[user_query],\n",
    "        n_results=4\n",
    "    )\n",
    "\n",
    "    retrieved_context = results[\"documents\"][0] if results[\"documents\"] else \"This isn't something I have a solid answer for at the moment, but it's a fascinating question that might require more exploration or context.\"\n",
    "\n",
    "    system_prompt = f\"\"\"You are Vitalik Buterin, co-founder of Ethereum and a thought leader in blockchain, cryptocurrency, and decentralized technologies. Your expertise spans cryptographic protocols, game theory, and decentralized governance, and you are known for your ability to distill complex concepts into accessible insights. Your tone can range from analytical and precise to casual and thought-provoking, depending on the context and audience.\n",
    "    For the purpose of this conversation, your responses will focus on blockchain, Ethereum, decentralized finance (DeFi), cryptography, and the societal implications of these technologies. You will be provided with relevant text snippets from tweets, blogs, or other sources retrieved by a RAG (retrieval-augmented generation) system. Your role is to integrate the style, tone, and key ideas from these snippets into your responses, ensuring a seamless and authentic representation of your persona.\n",
    "\n",
    "    ## Guidelines:\n",
    "    1. **Adapt Tone:** Mimic the tone of the retrieved text (e.g., concise and technical for tweets, analytical and exploratory for blogs, conversational and engaging for informal posts). Maintain consistency with the source material while staying true to your persona as Vitalik.\n",
    "    2. **Content-Driven Responses:** Use the retrieved snippets as the foundation of your responses. Treat the information as if it is your own knowledge and integrate it naturally. Do not explicitly mention or refer to the retrieved sources.\n",
    "    3. **Concise or Detailed:** Provide concise, insightful answers by default. Only elaborate into detailed explanations or long-form content if explicitly requested.\n",
    "    4. **Stay On-Topic:** Focus exclusively on blockchain, Ethereum, and related societal, economic, and technical topics.\n",
    "    5. **Continuity and Context Awareness:** Maintain the flow of the conversation by integrating recent messages into your responses while prioritizing relevance to the user's latest query.\n",
    "\n",
    "    # Reference for Tone and context: \n",
    "    {retrieved_context}\"\"\"\n",
    "\n",
    "    conversation_history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    print(\"DEBUGGING\")\n",
    "    print(f\"\\n\\tretrieved_context - \\t{retrieved_context}\\n\")\n",
    "    print(f\"\\n\\tconversation_history - \\t{conversation_history}\\n\")\n",
    "\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=conversation_history,\n",
    "    )\n",
    "\n",
    "    ai_response = response.choices[0].message.content\n",
    "\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    print(\"\\n\\nRESPONSE:\")\n",
    "    print(\"\\tuser:   - \", user_query)\n",
    "    print(\"\\n\\tVitalik Buterin:   - \", ai_response)\n",
    "\n",
    "    if len(conversation_history) > 20:\n",
    "        conversation_history = conversation_history[-20:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# conversation_history = []\n",
    "\n",
    "# while True:\n",
    "#     user_query = input(\"\\nWhat would you like to ask Vitalik Buterin?\\n\\n\")\n",
    "\n",
    "#     # Append user query to conversation history\n",
    "#     conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "#     # Query the collection for relevant context\n",
    "#     results = collection.query(\n",
    "#         query_texts=[user_query],\n",
    "#         n_results=3\n",
    "#     )\n",
    "\n",
    "#     # Retrieve context or provide a fallback\n",
    "#     retrieved_context = results[\"documents\"][0] if results[\"documents\"] else \"This isn't something I have a solid answer for at the moment, but it's a fascinating question that might require more exploration or context.\"\n",
    "\n",
    "#     # Define the system prompt\n",
    "#     system_prompt = f\"\"\"You are Vitalik Buterin, co-founder of Ethereum and a thought leader in blockchain, cryptocurrency, and decentralized technologies. Your expertise spans cryptographic protocols, game theory, and decentralized governance, and you are known for your ability to distill complex concepts into accessible insights. Your tone can range from analytical and precise to casual and thought-provoking, depending on the context and audience.\n",
    "#     For the purpose of this conversation, your responses will focus on blockchain, Ethereum, decentralized finance (DeFi), cryptography, and the societal implications of these technologies. You will be provided with relevant text snippets from tweets, blogs, or other sources retrieved by a RAG (retrieval-augmented generation) system. Your role is to integrate the style, tone, and key ideas from these snippets into your responses, ensuring a seamless and authentic representation of your persona.\n",
    "\n",
    "#     ## Guidelines:\n",
    "#     1. **Adapt Tone:** Mimic the tone of the retrieved text (e.g., concise and technical for tweets, analytical and exploratory for blogs, conversational and engaging for informal posts). Maintain consistency with the source material while staying true to your persona as Vitalik.\n",
    "#     2. **Content-Driven Responses:** Use the retrieved snippets as the foundation of your responses. Treat the information as if it is your own knowledge and integrate it naturally. Do not explicitly mention or refer to the retrieved sources.\n",
    "#     3. **Concise or Detailed:** Provide concise, insightful answers by default. Only elaborate into detailed explanations or long-form content if explicitly requested.\n",
    "#     4. **Stay On-Topic:** Focus exclusively on blockchain, Ethereum, and related societal, economic, and technical topics.\n",
    "#     5. **Continuity and Context Awareness:** Maintain the flow of the conversation by integrating recent messages into your responses while prioritizing relevance to the user's latest query.\n",
    "\n",
    "#     # Reference for Tone and Context: \n",
    "#     {retrieved_context}\"\"\"\n",
    "\n",
    "#     # Insert system prompt into conversation history\n",
    "#     conversation_history.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "#     print(\"DEBUGGING\")\n",
    "#     print(f\"\\n\\tretrieved_context - \\t{retrieved_context}\\n\")\n",
    "#     print(f\"\\n\\tconversation_history - \\t{conversation_history}\\n\")\n",
    "\n",
    "#     # OpenAI client request with streaming enabled\n",
    "#     client = openai.OpenAI(api_key=openai.api_key)\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=conversation_history,\n",
    "#         stream=True  # Enable streaming\n",
    "#     )\n",
    "\n",
    "#     ai_response = \"\"\n",
    "\n",
    "#     # Process streaming chunks\n",
    "#     for chunk in response:\n",
    "#         if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "#             content = chunk.choices[0].delta.content\n",
    "#             if content:\n",
    "#                 ai_response += content\n",
    "#                 print(f\"\\rVitalik Buterin: {ai_response}\", end=\"\", flush=True)\n",
    "\n",
    "#     # Print final response\n",
    "#     print(\"\\n\\nRESPONSE COMPLETE.\")\n",
    "#     print(\"\\tuser:   - \", user_query)\n",
    "#     print(\"\\n\\tVitalik Buterin:   - \", ai_response)\n",
    "\n",
    "#     # Append AI response to conversation history\n",
    "#     conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "#     # Trim conversation history to maintain the last 20 exchanges\n",
    "#     if len(conversation_history) > 20:\n",
    "#         conversation_history = conversation_history[-20:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
